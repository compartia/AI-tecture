{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMB2QLcsFGX8MyQke4kuIzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/AI-tecture/blob/dev/miro_openai_pipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Miro and Chat GPT\n",
        "\n",
        " status: **DRAFT**\n",
        "\n",
        " **by:** Artem Zaborskiy\n",
        "\n",
        "\n",
        " system that integrates AI-generated content with Miro (https://miro.com/), a collaborative online whiteboard platform. It specifically handles the part of the workflow where an AI-generated response is posted back to the Miro board, enhancing the interactive and dynamic capabilities of the board by incorporating AI insights directly into the collaborative workspace.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0TD4KZgQRolG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODOs:\n",
        "\n",
        "1. ~~sort nodes to process, proritize, what propmps to process first~~\n",
        "1. cache processing results,\n",
        "1. ~~check dates or checksums~~\n",
        "1. check for self-connected nodes\n",
        "1. check for prompt-prompt connectors\n",
        "1. do not reload the entire board after widget created: add new widget and its connector to dataframes\n",
        "1. ~~create output shape if propmpt node has no output connectors~~\n",
        "1. Update shapes chache data tables after AI answer is received"
      ],
      "metadata": {
        "id": "wZF7wgCUo_60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare OPEN AI client"
      ],
      "metadata": {
        "id": "mgh_Yl64GmYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML"
      ],
      "metadata": {
        "id": "DFDb4tbHI-Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai typing-extensions"
      ],
      "metadata": {
        "id": "mdlRmobQRvU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "ai_client = openai.OpenAI(api_key=userdata.get('OP_API')  )"
      ],
      "metadata": {
        "id": "dKAaHFKTRveu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOARD_ID = 'uXjVN61x8Pg='\n",
        "DEBUG_OPENAI = False"
      ],
      "metadata": {
        "id": "yWPQ26qRlSI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test open AI api calls"
      ],
      "metadata": {
        "id": "pdmuX6DrGCTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if DEBUG_OPENAI:\n",
        "  completion = ai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair. Your answer is in a form of list of <p> html tags.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # print(completion.choices[0].message)\n",
        "  display(HTML(completion.choices[0].message.content))"
      ],
      "metadata": {
        "id": "sByeqew3Gi9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if DEBUG_OPENAI:\n",
        "  completion = ai_client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair. Your answer is in a form of list of <p> html tags.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
        "    ]\n",
        "  )\n",
        "  #\n",
        "  # print(completion.choices[0].message)\n",
        "  display(HTML(completion.choices[0].message.content))"
      ],
      "metadata": {
        "id": "G4igP8-uGx-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if DEBUG_OPENAI:\n",
        "  completion = ai_client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair. Your answer is in a form of list of <p> html tags.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  display(HTML(completion.choices[0].message.content))"
      ],
      "metadata": {
        "id": "gyGRd7BKHAXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI API tools"
      ],
      "metadata": {
        "id": "N5aZVT3Ykouu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_ai(prompt, argument_context):\n",
        "\n",
        "  completion = ai_client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
        "      # {\"role\": \"system\", \"content\": f\"{prompt}. Your answer is in a form of list of <p> HTML tags and has NO newline '\\n' symbols.\"},\n",
        "      {\"role\": \"user\", \"content\": argument_context}\n",
        "    ]\n",
        "  )\n",
        "  print(completion.choices)\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "if DEBUG_OPENAI or True:\n",
        "  r = ask_ai('you are a female philosopher. Answer in 2 words.', 'what is the meaning of life?')\n",
        "  print(r)\n",
        "  display(HTML(r))\n",
        "  # display(HTML(r))"
      ],
      "metadata": {
        "id": "PRjlYd75JPHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG_OPENAI or True:\n",
        "  r = ask_ai('you are a male philosopher. Answer in 2 words.', 'what is the meaning of life?')\n",
        "  print(r)\n",
        "  display(HTML(r))\n",
        "  # display(HTML(r))"
      ],
      "metadata": {
        "id": "eWcC9jNJTsws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miro client\n",
        "https://developers.miro.com/reference/api-reference"
      ],
      "metadata": {
        "id": "MgnF85-Pk3Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class MiroClient:\n",
        "  def __init__(self, token, board_id):\n",
        "    self.board_id = board_id\n",
        "    self.token = token\n",
        "    self.base_url = f\"https://api.miro.com/v2/boards/{board_id}\"\n",
        "\n",
        "    self.headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"authorization\": f\"Bearer {self.token}\"\n",
        "    }\n",
        "    self.connectors_df:pd.DataFrame or None = None\n",
        "    self.shapes_df:pd.DataFrame or None = None\n",
        "\n",
        "\n",
        "  def get_items(self, cursor=None, limit=50):\n",
        "    limit_p = f'limit={limit}'\n",
        "\n",
        "    url = f\"{self.base_url}/items?{limit_p}\"\n",
        "    if cursor is not None:\n",
        "      url = f\"{self.base_url}/items?cursor={cursor}&{limit_p}\"\n",
        "\n",
        "    response = requests.get(url, headers=self.headers)\n",
        "    return response\n",
        "\n",
        "  def get_connectors(self, cursor=None, limit=50):\n",
        "    limit_p = f'limit={limit}'\n",
        "\n",
        "    url = f\"{self.base_url}/connectors?{limit_p}\"\n",
        "    if cursor is not None:\n",
        "      url = f\"{self.base_url}/connectors?cursor={cursor}&{limit_p}\"\n",
        "\n",
        "    response = requests.get(url, headers=self.headers)\n",
        "    return response\n",
        "\n",
        "\n",
        "  def get_all_pages(self, page_method):\n",
        "\n",
        "    cursor = None\n",
        "\n",
        "    max_steps = 30\n",
        "    step = 0\n",
        "    while True:\n",
        "      step += 1\n",
        "      response = page_method(cursor)\n",
        "      rj = response.json()\n",
        "      yield rj\n",
        "      cursor = rj.get('cursor')\n",
        "      if cursor is None or step > max_steps:\n",
        "        break\n",
        "\n",
        "\n",
        "miro_client = MiroClient(userdata.get('MIRO_TOOKEN'), BOARD_ID)\n"
      ],
      "metadata": {
        "id": "Z99dNYc8jOA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting all connectors"
      ],
      "metadata": {
        "id": "rXIxqwkx1RMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_connectors(page:dict):\n",
        "  # print(page)\n",
        "  for i in page['data']:\n",
        "    yield i['id'], i.get('startItem', {}).get('id'), i.get('endItem', {}).get('id')\n",
        "\n",
        "\n",
        "##-----\n",
        "## test it\n",
        "one_page_con = miro_client.get_connectors(limit=10).json()\n",
        "x = [i for i in decode_connectors(one_page_con)]\n",
        "x"
      ],
      "metadata": {
        "id": "_PBOp6pZwCau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_connectors(self):\n",
        "  for page in self.get_all_pages(self.get_connectors):\n",
        "    for i in decode_connectors(page):\n",
        "      yield i\n",
        "\n",
        "\n",
        "def all_connectors_as_df(self):\n",
        "  all_connectors_iter = self.get_all_connectors()\n",
        "  self.connectors_df = pd.DataFrame(all_connectors_iter, columns=['id','id_from', 'id_to'])\n",
        "  self.connectors_df = self.connectors_df.set_index('id')\n",
        "\n",
        "  return self.connectors_df\n",
        "\n",
        "\n",
        "MiroClient.get_all_connectors = get_all_connectors\n",
        "MiroClient.all_connectors_as_df = all_connectors_as_df\n",
        "\n",
        "\n",
        "\n",
        "##-----\n",
        "miro_client = MiroClient(userdata.get('MIRO_TOOKEN'), BOARD_ID)\n",
        "\n",
        "miro_client.all_connectors_as_df()\n",
        "miro_client.connectors_df\n"
      ],
      "metadata": {
        "id": "KQqR-FymxOHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting all shapes (widgets)"
      ],
      "metadata": {
        "id": "ChwakwI91NJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_items(page:dict):\n",
        "\n",
        "  for i in page['data']:\n",
        "    # print(type(i))\n",
        "    if type(i) == dict :\n",
        "      yield i['id'], i['type'], i['data'].get('shape', '_undefined_'), i['data'].get('content'), i['modifiedAt'], i['geometry'], i['position']\n",
        "\n",
        "    else:\n",
        "      print('NOT A DICT!!', type(i), i)\n",
        "      decode_items(i)\n",
        "\n",
        "\n",
        "\n",
        "def get_all_items(self):\n",
        "  for page in self.get_all_pages(self.get_items):\n",
        "    # print( page.get('cursor'), page.get('size'), page.get('total') )\n",
        "    for i in decode_items(page):\n",
        "      yield i\n",
        "      # print(i)\n",
        "\n",
        "\n",
        "MiroClient.get_all_items = get_all_items"
      ],
      "metadata": {
        "id": "KK69YBMzWrmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def all_items_as_df(miro_client):\n",
        "  items_iter = miro_client.get_all_items()\n",
        "\n",
        "  df = pd.DataFrame(items_iter, columns=['id','type', 'shape', \"contents\", 'modifiedAt', 'geometry', 'position'])\n",
        "  df = df.set_index('id')\n",
        "  df['modifiedAt_date'] = pd.to_datetime(df['modifiedAt'])\n",
        "\n",
        "  miro_client.shapes_df = df\n",
        "  return miro_client.shapes_df\n",
        "\n",
        "\n",
        "MiroClient.all_items_as_df = all_items_as_df\n",
        "# --------\n",
        "\n",
        "miro_client = MiroClient(userdata.get('MIRO_TOOKEN'), BOARD_ID)\n",
        "shapes_df = miro_client.all_items_as_df()"
      ],
      "metadata": {
        "id": "PL9z0V2WMs5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "miro_client.shapes_df"
      ],
      "metadata": {
        "id": "e9b4QVNkRXhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update board item"
      ],
      "metadata": {
        "id": "D2dKqEeDLfjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_shape_info(self, shape_id):\n",
        "  url = f\"{self.base_url}/shapes/{shape_id}\"\n",
        "  response = requests.get(url, headers=self.headers)\n",
        "  print(response.text)\n",
        "\n",
        "def get_stiky_note_info(self, shape_id):\n",
        "  url = f\"{self.base_url}/sticky_notes/{shape_id}\"\n",
        "  response = requests.get(url, headers=self.headers)\n",
        "  print(response.text)\n",
        "\n",
        "\n",
        "def update_item(self, shape_id, item_type, content=None, shape=None, color = None):\n",
        "\n",
        "  url = None\n",
        "\n",
        "  if content is None:\n",
        "    payload = {}\n",
        "  else:\n",
        "    payload = {\n",
        "        \"data\": {\n",
        "            \"content\": content\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "  if color is not None:\n",
        "    payload['style']={}\n",
        "    payload['style']['fillColor'] = color\n",
        "\n",
        "    if item_type!='sticky_note':\n",
        "      payload['style']['fillOpacity'] = \"1.0\"\n",
        "\n",
        "\n",
        "  if item_type=='sticky_note':\n",
        "    url = f\"{self.base_url}/sticky_notes/{shape_id}\"\n",
        "  elif item_type=='shape':\n",
        "    url = f\"{self.base_url}/shapes/{shape_id}\"\n",
        "    if shape is not None:\n",
        "      payload['data']['shape'] = shape\n",
        "\n",
        "  print(payload)\n",
        "\n",
        "\n",
        "  response = requests.patch(url, json=payload, headers=self.headers)\n",
        "  print(response.text)\n",
        "  return response\n",
        "\n",
        "\n",
        "\n",
        "MiroClient.update_item = update_item\n",
        "MiroClient.get_shape_info = get_shape_info\n",
        "MiroClient.get_stiky_note_info = get_stiky_note_info\n",
        "\n",
        "#-----\n",
        "\n",
        "# miro_client = MiroClient(userdata.get('MIRO_TOOKEN'), BOARD_ID)\n",
        "\n",
        "# if False:\n",
        "\n",
        "  # miro_client.get_stiky_note_info('3458764578213963472')\n",
        "# test_id = '3458764578216605194'\n",
        "# miro_client.update_item( test_id, item_type=shapes_df.at[test_id, 'type'], content=completion.choices[0].message.content,  color='#ff0000')"
      ],
      "metadata": {
        "id": "PLj1AZXaLdTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_connector(self, form, to, text):\n",
        "  import requests\n",
        "\n",
        "  url = f\"{self.base_url}/connectors\"\n",
        "\n",
        "  payload = {\n",
        "      \"startItem\": {\n",
        "          \"id\": form,\n",
        "          \"snapTo\": \"right\"\n",
        "      },\n",
        "      \"endItem\": {\n",
        "          \"id\": to,\n",
        "          \"snapTo\": \"auto\"\n",
        "      },\n",
        "      \"style\": { \"endStrokeCap\": \"arrow\" },\n",
        "      \"shape\": \"curved\",\n",
        "      \"captions\": [{ \"content\": text }]\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=self.headers)\n",
        "  return response\n",
        "\n",
        "\n",
        "def create_stiky_note(self, contents, pos):\n",
        "\n",
        "  url = f\"{self.base_url}/sticky_notes\"\n",
        "\n",
        "  payload = {\n",
        "    \"data\": {\n",
        "        \"content\": contents,\n",
        "        \"shape\": \"rectangle\"\n",
        "    },\n",
        "    \"style\": {\n",
        "        \"fillColor\": \"pink\",\n",
        "        \"textAlign\": \"left\"\n",
        "    },\n",
        "    \"position\": {\n",
        "        \"x\": pos[0],\n",
        "        \"y\": pos[1]\n",
        "    },\n",
        "    \"geometry\": {\n",
        "        \"height\": pos[2]\n",
        "        # \"width\": pos[3]\n",
        "    }\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=self.headers)\n",
        "  return response\n",
        "\n",
        "\n",
        "MiroClient.create_stiky_note = create_stiky_note\n",
        "MiroClient.create_connector = create_connector\n",
        "\n",
        "\n",
        "# --------\n",
        "\n",
        "def get_out_items(self, node_id):\n",
        "  _links = self.connectors_df[ self.connectors_df['id_from']==node_id]\n",
        "  return self.shapes_df.loc [ _links['id_to'] ]\n",
        "\n",
        "def get_in_items(self, node_id):\n",
        "  _links = self.connectors_df[ self.connectors_df['id_to']==node_id]\n",
        "  return self.shapes_df.loc [_links['id_from'] ]\n",
        "\n",
        "MiroClient.get_out_items=get_out_items\n",
        "MiroClient.get_in_items=get_in_items\n",
        "\n",
        "miro_client = MiroClient(userdata.get('MIRO_TOOKEN'), BOARD_ID)\n",
        "# response = miro_client.create_stiky_note( \"TESTCREATE\", (20,20,400,400) )\n",
        "# response.json()['id']"
      ],
      "metadata": {
        "id": "6KA8i3wcJBAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sorting nodes (Tokpologically) witn networkx"
      ],
      "metadata": {
        "id": "8ad077X_2RwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install networkx"
      ],
      "metadata": {
        "id": "yeSVbd692Qsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "miro_client = MiroClient(userdata.get('MIRO_TOOKEN'), BOARD_ID)\n",
        "\n",
        "miro_client.all_connectors_as_df()\n",
        "miro_client.all_items_as_df()"
      ],
      "metadata": {
        "id": "-ErmYjwsmYXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform a topological sort on the condensed DAG\n",
        "- https://arxiv.org/pdf/2110.07580.pdf\n",
        "\n",
        "- Step 1: Identify SCCs with NetworkX `networkx.strongly_connected_components(G)`.\n",
        "- Step 2: Contract (condense) each SCC into a single node, reducing the original graph into a DAG where each node represents an SCC. This is  called the **\"condensation\"** of the graph.\n",
        "- Step 3: Perform a topological sort on the condensed DAG. This gives an order of the SCCs, which can serve as a coarse **hierarchical ordering** of the original graph's components."
      ],
      "metadata": {
        "id": "RyeIKAbb4zNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for i, e in miro_client.connectors_df.iterrows():\n",
        "  # print (e)\n",
        "  G.add_edge(e['id_from'], e['id_to'])\n",
        "\n",
        "sccs = list(nx.strongly_connected_components(G))\n",
        "\n",
        "# Step 2: Create a condensed graph of SCCs\n",
        "condensed_graph = nx.condensation(G, sccs)\n",
        "for i, scc in enumerate(sccs):\n",
        "    # The nodes of the original graph in the current SCC\n",
        "    original_nodes = condensed_graph.nodes[i]['members']\n",
        "    if len(original_nodes)>1:\n",
        "      print(f\"SCC {i}: Contains original nodes {original_nodes}\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Perform a topological sort on the condensed graph\n",
        "topological_order_sccs = list(nx.topological_sort(condensed_graph))\n",
        "\n",
        "# Printing the order of SCCs\n"
      ],
      "metadata": {
        "id": "86_jvaOl2Eke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "scc_index = 0  # For example, to get the first SCC in the topological order\n",
        "# Retrieve the node set (members) of the original graph that form this SCC\n",
        "original_nodes_in_scc = condensed_graph.nodes[topological_order_sccs[scc_index]]['members']\n",
        "print(f\"Original nodes in SCC {scc_index}: {original_nodes_in_scc}\")"
      ],
      "metadata": {
        "id": "HlwFgRrGmp0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collect IDs"
      ],
      "metadata": {
        "id": "9XstiOs34p0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ordered_nodes = []\n",
        "for i in topological_order_sccs:\n",
        "  original_nodes_in_scc = condensed_graph.nodes[topological_order_sccs[i]]['members']\n",
        "  for n in original_nodes_in_scc:\n",
        "    ordered_nodes.append(n)\n",
        "    miro_client.shapes_df.at[n, 'order']=i\n",
        "\n",
        "ordered_shapes_df = miro_client.shapes_df.loc[ordered_nodes].sort_values(by='order')\n",
        "ordered_shapes_df.tail(4)"
      ],
      "metadata": {
        "id": "pCJcX-9h3jVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "haRxdHWe_00b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing the board\n",
        "Updating Miro board widgets with an answers generated by an AI model. Here's a breakdown of its functionality:"
      ],
      "metadata": {
        "id": "7fl6403nMD0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Collect shapes to process\n",
        "1. get all prompts\n",
        "2. get **linked** nodes\n",
        "3. **invalidate** nodes and prompts that updated later than answer nodes\n",
        "4. **prioritize** nodes\n",
        "5. **Update Widget**:  the target widget(s) is updated with AI-generated answer. This update is performed by calling the `update_item` method of the `miro_client` object"
      ],
      "metadata": {
        "id": "oiG-BLkj4bh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. get all prompts"
      ],
      "metadata": {
        "id": "9vssKw8ypY0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_df = ordered_shapes_df[ordered_shapes_df['shape']=='parallelogram'].sort_values(by='order')\n",
        "prompts_df"
      ],
      "metadata": {
        "id": "PVVnxy9CWYf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### create missing outputs"
      ],
      "metadata": {
        "id": "Z9VPiK30q0Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_contents='waiting for AI to answer ....'"
      ],
      "metadata": {
        "id": "hexEiO1tNx4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def _build_output_shape(shape_id, contents = default_contents):\n",
        "  shapes_df = miro_client.shapes_df\n",
        "  shape = shapes_df.loc [ shape_id ]\n",
        "\n",
        "  pos = shape['position']\n",
        "  geo = shape['geometry']\n",
        "\n",
        "  size = max(geo['width'], geo['height'])\n",
        "\n",
        "  response = miro_client.create_stiky_note( contents, (pos['x'] +  geo['width'] + size, pos['y'], size, size) )\n",
        "  a_id = response.json()['id']\n",
        "\n",
        "  response = miro_client.create_connector( shape_id,  a_id, 'AI answer' )\n",
        "\n",
        "  return a_id\n",
        "\n",
        "# test------\n",
        "# build_output_shape('3458764578079690837', \"TESTCREATE\")\n",
        "\n",
        "\n",
        "_needs_reload = False\n",
        "for prompt_id in prompts_df.index:\n",
        "  out_links = miro_client.connectors_df[ miro_client.connectors_df['id_from']==prompt_id]\n",
        "  if len(out_links) == 0:\n",
        "    out_shape_id = _build_output_shape(prompt_id, contents=default_contents)\n",
        "    _needs_reload = True\n",
        "\n",
        "if _needs_reload:\n",
        "  #TODO: OPTIMIZE!!\n",
        "  print ('reloading board')\n",
        "  miro_client.all_items_as_df()\n",
        "  miro_client.all_connectors_as_df()\n",
        "\n",
        "del _needs_reload"
      ],
      "metadata": {
        "id": "W-fQUh3xw_kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: descendants = nx.descendants(G, start_node)\n",
        "\n",
        "\n",
        "- Identify which prompts (or shapes) need to be updated based on the **modification dates** of incoming and outgoing items. If a prompt is older than any item feeding into it, it is considered outdated.\n",
        "\n",
        "- Determine the impact of these updates, using a directed graph to find all **descendants** of the outdated prompts. This implies that any changes to a prompt potentially invalidate all items that are downstream from it in the workflow or dependency graph.\n",
        "\n",
        "- Compile a list of shapes that need to be updated"
      ],
      "metadata": {
        "id": "q7Qvam9WC4bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  a set to keep track of IDs that need to be updated\n",
        "invalidated_ids = set()\n",
        "\n",
        "# Iterate over each prompt ID\n",
        "for prompt_id in prompts_df.index:\n",
        "\n",
        "  # Retrieve outgoing/incoming items related to the current prompt ID\n",
        "  out_items =  miro_client.get_out_items(prompt_id)\n",
        "  in_items =  miro_client.get_in_items(prompt_id)\n",
        "\n",
        "  # DATES\n",
        "  # Find the maximum/min modification date among incoming/outgoing items\n",
        "  in_date = in_items.modifiedAt_date.max()\n",
        "  out_date = out_items.modifiedAt_date.min()\n",
        "\n",
        "  # Get the modification date of the current prompt\n",
        "  prompt_date = miro_client.shapes_df.at[prompt_id, 'modifiedAt_date']\n",
        "\n",
        "  # Check if the current prompt is older than any of the incoming items\n",
        "  if (prompt_date < in_date) or (prompt_date > out_date):\n",
        "    # If so, find all descendants of the prompt ID in the graph (items affected by changes)\n",
        "    descendants = nx.descendants(G, prompt_id)\n",
        "    for d in descendants:\n",
        "      # Add the descendant ID to the set of invalidated IDs\n",
        "      invalidated_ids.add(d)\n",
        "\n",
        "\n",
        "    # This line is commented out; it seems it was an alternative way to add out_items to invalidated_ids\n",
        "\n",
        "# Select shapes from a DataFrame that match the invalidated IDs\n",
        "shapes_to_update = ordered_shapes_df.loc[invalidated_ids]\n",
        "# Filter out shapes that are not 'parallelogram' from the shapes to update\n",
        "# shapes_to_update = shapes_to_update[shapes_to_update['shape'] != 'parallelogram']\n",
        "shapes_to_update"
      ],
      "metadata": {
        "id": "rjm84TPVnnoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Render invalidated shapes as red"
      ],
      "metadata": {
        "id": "ea4W_fcHgdqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_color_for_state(out_shape_id, state):\n",
        "  item_type = miro_client.shapes_df.at[out_shape_id, 'type']\n",
        "\n",
        "  if state==0:\n",
        "    if item_type=='sticky_note':\n",
        "      color = 'red'\n",
        "    else:\n",
        "      color = '#ff5566'\n",
        "\n",
        "  if state==1:\n",
        "    if item_type=='sticky_note':\n",
        "      color = 'light_green'\n",
        "    else:\n",
        "      color = '#ccffdd'\n",
        "\n",
        "  return color"
      ],
      "metadata": {
        "id": "95xsW7zaNOW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_outputs = shapes_to_update[shapes_to_update['shape'] != 'parallelogram']\n",
        "for out_shape_id, item in _outputs.iterrows():\n",
        "  color = get_color_for_state(out_shape_id, 0)\n",
        "  res = miro_client.update_item( out_shape_id, item_type=item['type'], content=f\"<p><i>order: {int(item['order'])}</i></p>   {default_contents}\", color = color)\n",
        "del _outputs"
      ],
      "metadata": {
        "id": "CtiGpo7ENOwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calling AI for answers"
      ],
      "metadata": {
        "id": "3dL3BHDNglf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_shape_id(prompt_id):\n",
        "  out_links = miro_client.connectors_df[ miro_client.connectors_df['id_from']==prompt_id]\n",
        "  if len(out_links) > 0:\n",
        "    _items = miro_client.shapes_df.loc[out_links['id_to'] ]\n",
        "    return _items.iloc[0].name\n",
        "\n",
        "def get_incoming_text(prompt_id):\n",
        "\n",
        "  incoming_items = miro_client.get_in_items(prompt_id).sort_values(by='position', key=lambda x: x.map(lambda d: d['y']), ascending=True)\n",
        "  concatenated_text = '\\n\\n'.join(incoming_items.contents)\n",
        "\n",
        "  return concatenated_text"
      ],
      "metadata": {
        "id": "cGzaDa0XpL66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_answer(prompt_id):\n",
        "\n",
        "  connectors_df = miro_client.connectors_df\n",
        "  shapes_df = miro_client.shapes_df\n",
        "  incoming_links = connectors_df[ connectors_df['id_to']==prompt_id]\n",
        "  out_shape_id = get_output_shape_id(prompt_id)\n",
        "\n",
        "\n",
        "\n",
        "  if out_shape_id in invalidated_ids:\n",
        "\n",
        "    _propmt_text = shapes_df.loc[prompt_id]['contents']\n",
        "    in_text = get_incoming_text(prompt_id)\n",
        "\n",
        "    _item_type = shapes_df.at[out_shape_id, 'type']\n",
        "    answer_text = ask_ai(_propmt_text, in_text)\n",
        "\n",
        "    try:\n",
        "      display(HTML(answer_text))\n",
        "    except:\n",
        "      #it cannot display malformed HTML\n",
        "      print('cannot render html', answer_text)\n",
        "\n",
        "    color = get_color_for_state(out_shape_id, 1)\n",
        "\n",
        "    #update cache with AI answer\n",
        "    shapes_df.at[out_shape_id, 'contents'] = answer_text\n",
        "    res = miro_client.update_item( out_shape_id, item_type=_item_type, content=answer_text,  shape='round_rectangle', color = color)\n",
        "\n",
        "  return answer_text\n",
        "\n",
        "\n",
        "\n",
        "# propmpts are sorted\n",
        "for prompt_id in prompts_df.index:\n",
        "  print('-'*40)\n",
        "  print(prompt_id, 'processing')\n",
        "  answer_text = build_answer(prompt_id)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kizif0PUoNHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nHAvtiOOg4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LRYBVc2ROoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rR-k1i55ls2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fh2sdLJyGiWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}